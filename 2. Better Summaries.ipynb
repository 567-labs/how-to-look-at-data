{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Better Summaries: Building Domain-Specific Clustering\n\n> **Series Overview**: This is the second notebook in our three-part series on systematically analyzing and improving RAG systems. In the first notebook, we discovered query patterns but found limitations with generic summaries. Now we'll fix that.\n\n> **Prerequisites**: Complete \"1. Cluster Conversations\" notebook first. You'll need the same dependencies and `GOOGLE_API_KEY` from the previous notebook.\n\n## Why This Matters\n\n**The generic summaries from our initial clustering missed crucial details that would enable effective query understanding.** When working with specialized domains like machine learning experiment tracking, generic descriptions like \"user seeks information about tracking\" fail to capture the specific W&B features, user goals, and pain points that matter for system improvement.\n\n**Custom summarization transforms vague descriptions into precise, actionable insights.** Instead of \"user requests assistance with tool integration,\" we can generate \"user is configuring W&B Artifacts for model versioning in PyTorch workflows.\" This precision is critical for building clusters that truly reflect how users interact with your platform.\n\nDomain-specific summaries enable us to:\n\n1. **Capture exact features** users are working with (Artifacts, Configs, Reports)\n2. **Identify specific goals** and pain points rather than generic categories  \n3. **Reveal usage patterns** that generic summaries obscure\n4. **Create foundations** for more targeted system improvements\n\n## What You'll Learn\n\nIn this notebook, you'll discover how to:\n\n1. **Build Custom Summary Models**\n   - Design specialized prompts that extract domain-specific information\n   - Implement length constraints for focused, consistent summaries\n   - Replace Kura's default summarization with your custom approach\n\n2. **Compare Summarization Approaches**\n   - Analyze the limitations of generic vs. domain-specific summaries\n   - See how improved summaries change clustering outcomes\n   - Understand the impact of summary quality on cluster interpretability\n\n3. **Generate Enhanced Clusters**\n   - Apply custom summaries to create more representative topic groups\n   - Configure clustering parameters for optimal domain-specific results\n   - Extract actionable insights about user behavior patterns\n\n## What You'll Discover\n\n**By the end of this notebook, you'll transform your nine generic clusters into three highly actionable categories**: Access Controls (data export/security), Deployment (service integration/auth), and Experiment Management (artifacts/visualization/multi-GPU). This dramatic improvement in cluster quality\u2014from vague topics to specific, actionable user needs\u2014will provide the foundation for building production classifiers in the next notebook.\n\n## The Power of Domain-Specific Clustering\n\n**While generic clustering tells you \"what\" users are asking about, domain-specific clustering reveals \"why\" and \"how\" they're struggling.** This shift from surface-level topics to deep user intent understanding is what enables you to build targeted solutions rather than generic improvements.\n\nBy the end of this series, you'll have a complete framework for turning raw user queries into systematic, data-driven RAG improvements that address real user needs rather than perceived ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Custom Summary Model\n",
        "\n",
        "To address the limitations we identified in our default summaries, we'll now implement our own custom summary model specific to Weights & Biases queries. By replacing the generic summarization approach with a domain-tailored solution, we can generate summaries that precisely capture the tools, features, and goals relevant to W&B users.\n",
        "\n",
        "The `WnBSummaryModel` class we'll create extends Kura's base `SummaryModel` with a specialized prompt that instructs the model to:\n",
        "\n",
        "1. Identify specific W&B features mentioned in the query (e.g., Artifacts, Configs, Reports)\n",
        "2. Clearly state the problem the user is trying to solve\n",
        "3. Format responses concisely (25 words or less) to ensure summaries remain focused\n",
        "\n",
        "This approach generates summaries that are not only more informative but also more consistent, making them ideal building blocks for meaningful clustering. Let's implement our custom model and see how it transforms our understanding of user query patterns.\n",
        "\n",
        "### Loading in Conversation\n",
        "\n",
        "Let's first start by loading in our conversations and parsing it into a list of `Conversation` objects that `Kura` can work with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ivanleo/Documents/coding/chroma-workshop/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from lib.conversation import process_query_obj\n",
        "import json\n",
        "\n",
        "with open(\"./data/conversations.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "conversations = [process_query_obj(obj) for obj in data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now try to see how our default summaries look like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Summarising 2 conversations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chat_id='5e878c76-25c1-4bad-8cae-6a40ca4c8138' summary=\"The user's overall request for the assistant is to explain how to track machine learning experiments using a specific library by creating a run, storing hyperparameters, logging metrics, and saving outputs of the run as demonstrated in the pseudocode provided .\" metadata={'conversation_turns': 1, 'query_id': '5e878c76-25c1-4bad-8cae-6a40ca4c8138'}\n",
            "chat_id='d7b77e8a-e86c-4953-bc9f-672618cdb751' summary=\"The user's overall request for the assistant is to summarize information about Bayesian optimization, a hyperparameter tuning technique, and its implementation in Python using libraries like bayes_opt.\" metadata={'conversation_turns': 1, 'query_id': 'd7b77e8a-e86c-4953-bc9f-672618cdb751'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from kura.summarisation import SummaryModel\n",
        "\n",
        "summaries = await SummaryModel().summarise(conversations[:2])\n",
        "for summary in summaries:\n",
        "    print(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at these default summaries, we can identify several key limitations that prevent them from being truly useful for clustering W&B-specific queries:\n",
        "\n",
        "**Problems with Default Summaries**\n",
        "\n",
        "1. Lack of Specificity: The first summary refers to \"a specific tool\" rather than explicitly naming Weights & Biases, missing the opportunity to highlight the domain context.\n",
        "\n",
        "2. Missing Feature Details: Neither summary identifies which specific W&B features the users are interested in (experiment tracking, Bayesian optimization for hyperparameter tuning), which would be crucial for meaningful clustering.\n",
        "\n",
        "These generic summaries would lead to clusters based primarily on query structure (\"users asking for information\") rather than meaningful W&B feature categories or user goals. \n",
        "\n",
        "By defining our own summarisation model, we can address these limitations and cluster our user queries based off the specific problems and features they are trying to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Our New Summary Model\n",
        "\n",
        "Let's now define a new `WnBSummaryModel` which will help address the shortcomings of the default summarisation model.\n",
        "\n",
        "We'll do so by modifying the `summarise_conversation` method so that our summaries can become more precise and feature-focused. This allows us to better reflect how users interact with Weights and Biases and in turn translate to more representative clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kura.types import Conversation, ConversationSummary\n",
        "from kura.summarisation import SummaryModel, GeneratedSummary\n",
        "\n",
        "\n",
        "class WnBSummaryModel(SummaryModel):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    async def summarise_conversation(\n",
        "        self, conversation: Conversation\n",
        "    ) -> ConversationSummary:\n",
        "        # Get the default client and semaphore - This is going to be the Gemini GenAI client and a semaphore limit of around 50 concurrent requests \n",
        "        client = self.clients.get(\"default\")  # type: ignore\n",
        "        sem = self.sems.get(\"default\")  # type: ignore\n",
        "\n",
        "        async with sem:\n",
        "            resp = await client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": \"\"\"\n",
        "Summarize the user's issue based on their query and the retrieved information from the Weights and Biases FAQ section.\n",
        "\n",
        "In your response:\n",
        "\n",
        "1. Identify the specific Weights and Biases feature(s) the user is working with (e.g., Artifacts, Configs, Reports), including any features implied but not directly named.\n",
        "\n",
        "2. Clearly state the problem they're trying to solve.\n",
        "\n",
        "Format your response in 25 words or less following these patterns:\n",
        "\n",
        "If the query has a clear feature and problem:\n",
        "\"The user is using Weights and Biases's [feature(s)] to [problem] and needs help with [specific issue].\"\n",
        "\n",
        "If the query is ambiguous (e.g., \"Bayesian optimization\" without context):\n",
        "\"The user made a query about [topic].\"\n",
        "\n",
        "Analyze both the query and retrieved documents carefully to identify the user's actual goal rather than just repeating their keywords. Here is the message context that you should refer to:\n",
        "<context>\n",
        "{{ context }}\n",
        "</context>\n",
        "\n",
        "Be as specific as possible in your response.\n",
        "\"\"\",\n",
        "                    },\n",
        "                ],\n",
        "                response_model=GeneratedSummary,\n",
        "                context={\"context\": conversation.messages[0].content},\n",
        "            )\n",
        "\n",
        "            return ConversationSummary(\n",
        "                chat_id=conversation.chat_id,\n",
        "                summary=resp.summary,\n",
        "                metadata={\n",
        "                    \"conversation_turns\": len(conversation.messages),\n",
        "                },\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now see the generated summaries by calling the `summarise` method below. We'll be using the same conversations above which we generated summaries for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Summarising 2 conversations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.01it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ConversationSummary</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">chat_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'5e878c76-25c1-4bad-8cae-6a40ca4c8138'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">summary</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The user is using Weights and Biases' experiment tracking features to track machine learning </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">experiments, including logging metrics, saving hyperparameters, and saving model artifacts, and needs help </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">understanding the overall workflow.\"</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'conversation_turns'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mConversationSummary\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mchat_id\u001b[0m=\u001b[32m'5e878c76-25c1-4bad-8cae-6a40ca4c8138'\u001b[0m,\n",
              "    \u001b[33msummary\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m user is using Weights and Biases' experiment tracking features to track machine learning \u001b[0m\n",
              "\u001b[32mexperiments, including logging metrics, saving hyperparameters, and saving model artifacts, and needs help \u001b[0m\n",
              "\u001b[32munderstanding the overall workflow.\"\u001b[0m,\n",
              "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'conversation_turns'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ConversationSummary</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">chat_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'d7b77e8a-e86c-4953-bc9f-672618cdb751'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">summary</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The user is exploring Bayesian optimization for hyperparameter tuning within Weights &amp; Biases and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">needs guidance on its implementation and usage.'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'conversation_turns'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mConversationSummary\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mchat_id\u001b[0m=\u001b[32m'd7b77e8a-e86c-4953-bc9f-672618cdb751'\u001b[0m,\n",
              "    \u001b[33msummary\u001b[0m=\u001b[32m'The user is exploring Bayesian optimization for hyperparameter tuning within Weights & Biases and \u001b[0m\n",
              "\u001b[32mneeds guidance on its implementation and usage.'\u001b[0m,\n",
              "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'conversation_turns'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "summaries = await WnBSummaryModel().summarise(conversations[:2])\n",
        "for summary in summaries:\n",
        "    print(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering with Enhanced Summaries\n",
        "\n",
        "Now that we've developed a more domain-specific summarization approach tailored to the Weights & Biases ecosystem, we can apply these improved summaries to our clustering process. \n",
        "\n",
        "Our custom `WnBSummaryModel` captures the specific features, workflows, and user intentions that were missing in the default summaries, providing a stronger foundation for meaningful topic discovery.\n",
        "\n",
        "This will help us to reveal patterns in feature usage, common pain points and documentation gaps that might have been obscured in our analysis in our previous notebook. Let's see this in action below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Summarising 560 conversations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 560/560 [00:15<00:00, 35.40it/s]\n",
            "Embedding Summaries: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 560/560 [00:05<00:00, 100.27it/s]\n",
            "Generating Base Clusters: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 56/56 [00:03<00:00, 15.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting with 56 clusters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding Clusters: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 56/56 [00:01<00:00, 45.32it/s]\n",
            "Generating Meta Clusters: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:06<00:00,  1.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 27 clusters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding Clusters: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 27/27 [00:00<00:00, 27.52it/s]\n",
            "Generating Meta Clusters: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:05<00:00,  1.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 22 clusters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding Clusters: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22/22 [00:01<00:00, 20.16it/s]\n",
            "Generating Meta Clusters: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:04<00:00,  2.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 11 clusters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding Clusters: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:00<00:00, 23.41it/s]\n",
            "Generating Meta Clusters: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:03<00:00,  3.56s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 6 clusters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding Clusters: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:01<00:00,  4.37it/s]\n",
            "Generating Meta Clusters: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:03<00:00,  3.98s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 6 clusters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding Clusters: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00<00:00, 13.74it/s]\n",
            "Generating Meta Clusters: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:03<00:00,  3.33s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 3 clusters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ivanleo/Documents/coding/chroma-workshop/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from kura import Kura\n",
        "\n",
        "kura = Kura(\n",
        "    summarisation_model=WnBSummaryModel(),\n",
        "    max_clusters=5,\n",
        "    checkpoint_dir=\"./checkpoints_2\"\n",
        ")\n",
        "\n",
        "clusters = await kura.cluster_conversations(conversations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(d3629de048f44355abd533566718fbca) Manage and Debug Machine Learning Experiments with Weights &amp; Biases</span> : The \n",
              "clusters involve using Weights &amp; Biases <span style=\"font-weight: bold\">(</span>wandb<span style=\"font-weight: bold\">)</span> for experiment tracking, metric logging, model versioning, \n",
              "hyperparameter optimization, and integration with various machine learning models and frameworks, debugging, \n",
              "resolving errors, and customizing charts, plots, and reports, to gain insights into model training process, \n",
              "performance and optimize experiment runs.. : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">497</span>\n",
              "\n",
              "  \u2022 <span style=\"font-weight: bold\">Manage machine learning experiments with Weights &amp; Biases</span> : The clusters involve using Weights &amp; Biases <span style=\"font-weight: bold\">(</span>wandb<span style=\"font-weight: bold\">)</span>\n",
              "for experiment tracking, metric logging, model versioning, hyperparameter optimization, and integration with \n",
              "various machine learning models and frameworks, including best practices for naming, grouping, and logging data, \n",
              "and troubleshooting integration issues, to gain insights into model training process and performance and optimize \n",
              "experiment runs.. : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">264</span>\n",
              "    + <span style=\"font-weight: bold\">Use Weights &amp; Biases for machine learning experiment management</span> : The clusters involve using Weights &amp; Biases\n",
              "<span style=\"font-weight: bold\">(</span>wandb<span style=\"font-weight: bold\">)</span> for experiment tracking, metric logging, model versioning, hyperparameter optimization, and integration \n",
              "with various machine learning models and frameworks, including best practices for naming, grouping, and logging \n",
              "data, and troubleshooting integration issues, to gain insights into model training process and performance and \n",
              "optimize experiment runs.. : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">264</span>\n",
              "\n",
              "\n",
              "  \u2022 <span style=\"font-weight: bold\">Manage and analyze machine learning models with Weights &amp; Biases</span> : The clusters involve using Weights &amp; Biases \n",
              "<span style=\"font-weight: bold\">(</span>W&amp;B<span style=\"font-weight: bold\">)</span> for machine learning model management, experiment tracking, versioning, and artifact analysis, including \n",
              "storing, logging, querying, and integrating them into workflows, along with manipulating and analyzing Weights &amp; B \n",
              "tables using Pandas : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">136</span>\n",
              "    + <span style=\"font-weight: bold\">Manage and analyze machine learning models using Weights &amp; Biases</span> : The clusters involve using Weights &amp; \n",
              "Biases <span style=\"font-weight: bold\">(</span>W&amp;B<span style=\"font-weight: bold\">)</span> for machine learning model management, experiment tracking, versioning, and artifact analysis, \n",
              "including storing, logging, querying, and integrating them into workflows, along with manipulating and analyzing \n",
              "Weights &amp; B tables using Pandas : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">136</span>\n",
              "\n",
              "\n",
              "  \u2022 <span style=\"font-weight: bold\">Debug and Customize Weights &amp; Biases Experiments</span> : The clusters involve users seeking assistance with \n",
              "debugging, resolving errors, and customizing charts, plots, and reports in Weights &amp; Biases, including logging \n",
              "metrics, tracking data, managing GPU usage, and resolving errors during machine learning experiments and \n",
              "hyperparameter optimization, and programmatically accessing runs and their attributes : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">97</span>\n",
              "    + <span style=\"font-weight: bold\">Debug and Customize Weights &amp; Biases Experiments and Visualizations</span> : The clusters involve users seeking \n",
              "assistance with debugging, resolving errors, and customizing charts, plots, and reports in Weights &amp; Biases, \n",
              "including logging metrics, tracking data, managing GPU usage, and resolving errors during machine learning \n",
              "experiments and hyperparameter optimization, and programmatically accessing runs and their attributes : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">97</span>\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "====\n",
              "\n",
              "\n",
              "<span style=\"font-weight: bold\">(461d0109b1134f8cadf7fb3a4d5818df) Integrate Weights &amp; Biases with cloud services and Docker images</span> : Integrate \n",
              "Weights &amp; Biases with cloud services and Docker images: Users needed assistance with Weights &amp; Biases \n",
              "authentication, particularly in cloud environments like AWS SageMaker, and with integrations involving custom \n",
              "Docker images. They sought guidance on secure key management, service accounts, and configuring SSO on the \n",
              "server-side W&amp;B app, as well as examples for setting up W&amp;B Launch with SageMaker and custom images <span style=\"font-weight: bold\">(</span>BYOI<span style=\"font-weight: bold\">)</span>. : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
              "\n",
              "  \u2022 <span style=\"font-weight: bold\">Integrate Weights &amp; Biases with cloud services and Docker images</span> : Users needed assistance with Weights &amp; \n",
              "Biases authentication, particularly in cloud environments like AWS SageMaker, and with integrations involving \n",
              "custom Docker images. They sought guidance on secure key management, service accounts, and configuring SSO on the \n",
              "server-side W&amp;B app, as well as examples for setting up W&amp;B Launch with SageMaker and custom images <span style=\"font-weight: bold\">(</span>BYOI<span style=\"font-weight: bold\">)</span>. : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
              "    + <span style=\"font-weight: bold\">Configure and Integrate W&amp;B with Cloud Services</span> : Users needed assistance with Weights &amp; Biases \n",
              "authentication, particularly in cloud environments like AWS SageMaker, and with integrations involving custom \n",
              "Docker images. They sought guidance on secure key management, service accounts, and configuring SSO on the \n",
              "server-side W&amp;B app, as well as examples for setting up W&amp;B Launch with SageMaker and custom images <span style=\"font-weight: bold\">(</span>BYOI<span style=\"font-weight: bold\">)</span>. : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "====\n",
              "\n",
              "\n",
              "<span style=\"font-weight: bold\">(93cbee975f974856ba0e707921dcc2d0) Manage and export data from Weights &amp; Biases</span> : The clusters describe managing \n",
              "and exporting data, artifacts, access control, and permissions from the Weights &amp; Biases platform, including team \n",
              "roles, collaboration features, and data formats, using the web panel or API, sometimes without an API key. It also \n",
              "includes understanding team roles, collaboration features, setting permissions, controlling access, and enabling \n",
              "collaboration with both W&amp;B users and non-users, including handling sensitive data and resolving edit conflicts for\n",
              "multi-user access, admin roles, and SSO configurations : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">53</span>\n",
              "\n",
              "  \u2022 <span style=\"font-weight: bold\">Manage access control and permissions for Weights &amp; Biases.</span> : The clusters describe managing and understanding \n",
              "weights and biases teams, projects, platform features, access control, permissions, and sharing capabilities within\n",
              "the Weights &amp; Biases platform. It also includes understanding team roles, collaboration features, setting \n",
              "permissions, controlling access, and enabling collaboration with both W&amp;B users and non-users, including handling \n",
              "sensitive data and resolving edit conflicts for multi-user access, admin roles, and SSO configurations.  : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>\n",
              "    + <span style=\"font-weight: bold\">Control access and permissions in Weights &amp; Biases</span> : The clusters describe managing and understanding weights\n",
              "and biases teams, projects, platform features, access control, permissions, and sharing capabilities within the \n",
              "Weights &amp; Biases platform. It also includes understanding team roles, collaboration features, setting permissions, \n",
              "controlling access, and enabling collaboration with both W&amp;B users and non-users, including handling sensitive data\n",
              "and resolving edit conflicts for multi-user access, admin roles, and SSO configurations.  : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>\n",
              "\n",
              "\n",
              "  \u2022 <span style=\"font-weight: bold\">Export data and artifacts from Weights &amp; Biases</span> : The clusters involve users seeking assistance with exporting \n",
              "various data formats and downloading artifacts from the Weights &amp; Biases platform, often through the web panel or \n",
              "API, sometimes without an API key : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>\n",
              "    + <span style=\"font-weight: bold\">Export data and artifacts from Weights &amp; Biases</span> : The clusters involve users seeking assistance with \n",
              "exporting various data formats and downloading artifacts from the Weights &amp; Biases platform, often through the web \n",
              "panel or API, sometimes without an API key : <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "====\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m(\u001b[0m\u001b[1md3629de048f44355abd533566718fbca\u001b[0m\u001b[1m)\u001b[0m\u001b[1m Manage and Debug Machine Learning Experiments with Weights & Biases\u001b[0m : The \n",
              "clusters involve using Weights & Biases \u001b[1m(\u001b[0mwandb\u001b[1m)\u001b[0m for experiment tracking, metric logging, model versioning, \n",
              "hyperparameter optimization, and integration with various machine learning models and frameworks, debugging, \n",
              "resolving errors, and customizing charts, plots, and reports, to gain insights into model training process, \n",
              "performance and optimize experiment runs.. : \u001b[1;36m497\u001b[0m\n",
              "\n",
              "  \u2022 \u001b[1mManage machine learning experiments with Weights & Biases\u001b[0m : The clusters involve using Weights & Biases \u001b[1m(\u001b[0mwandb\u001b[1m)\u001b[0m\n",
              "for experiment tracking, metric logging, model versioning, hyperparameter optimization, and integration with \n",
              "various machine learning models and frameworks, including best practices for naming, grouping, and logging data, \n",
              "and troubleshooting integration issues, to gain insights into model training process and performance and optimize \n",
              "experiment runs.. : \u001b[1;36m264\u001b[0m\n",
              "    + \u001b[1mUse Weights & Biases for machine learning experiment management\u001b[0m : The clusters involve using Weights & Biases\n",
              "\u001b[1m(\u001b[0mwandb\u001b[1m)\u001b[0m for experiment tracking, metric logging, model versioning, hyperparameter optimization, and integration \n",
              "with various machine learning models and frameworks, including best practices for naming, grouping, and logging \n",
              "data, and troubleshooting integration issues, to gain insights into model training process and performance and \n",
              "optimize experiment runs.. : \u001b[1;36m264\u001b[0m\n",
              "\n",
              "\n",
              "  \u2022 \u001b[1mManage and analyze machine learning models with Weights & Biases\u001b[0m : The clusters involve using Weights & Biases \n",
              "\u001b[1m(\u001b[0mW&B\u001b[1m)\u001b[0m for machine learning model management, experiment tracking, versioning, and artifact analysis, including \n",
              "storing, logging, querying, and integrating them into workflows, along with manipulating and analyzing Weights & B \n",
              "tables using Pandas : \u001b[1;36m136\u001b[0m\n",
              "    + \u001b[1mManage and analyze machine learning models using Weights & Biases\u001b[0m : The clusters involve using Weights & \n",
              "Biases \u001b[1m(\u001b[0mW&B\u001b[1m)\u001b[0m for machine learning model management, experiment tracking, versioning, and artifact analysis, \n",
              "including storing, logging, querying, and integrating them into workflows, along with manipulating and analyzing \n",
              "Weights & B tables using Pandas : \u001b[1;36m136\u001b[0m\n",
              "\n",
              "\n",
              "  \u2022 \u001b[1mDebug and Customize Weights & Biases Experiments\u001b[0m : The clusters involve users seeking assistance with \n",
              "debugging, resolving errors, and customizing charts, plots, and reports in Weights & Biases, including logging \n",
              "metrics, tracking data, managing GPU usage, and resolving errors during machine learning experiments and \n",
              "hyperparameter optimization, and programmatically accessing runs and their attributes : \u001b[1;36m97\u001b[0m\n",
              "    + \u001b[1mDebug and Customize Weights & Biases Experiments and Visualizations\u001b[0m : The clusters involve users seeking \n",
              "assistance with debugging, resolving errors, and customizing charts, plots, and reports in Weights & Biases, \n",
              "including logging metrics, tracking data, managing GPU usage, and resolving errors during machine learning \n",
              "experiments and hyperparameter optimization, and programmatically accessing runs and their attributes : \u001b[1;36m97\u001b[0m\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "====\n",
              "\n",
              "\n",
              "\u001b[1m(\u001b[0m\u001b[1m461d0109b1134f8cadf7fb3a4d5818df\u001b[0m\u001b[1m)\u001b[0m\u001b[1m Integrate Weights & Biases with cloud services and Docker images\u001b[0m : Integrate \n",
              "Weights & Biases with cloud services and Docker images: Users needed assistance with Weights & Biases \n",
              "authentication, particularly in cloud environments like AWS SageMaker, and with integrations involving custom \n",
              "Docker images. They sought guidance on secure key management, service accounts, and configuring SSO on the \n",
              "server-side W&B app, as well as examples for setting up W&B Launch with SageMaker and custom images \u001b[1m(\u001b[0mBYOI\u001b[1m)\u001b[0m. : \u001b[1;36m10\u001b[0m\n",
              "\n",
              "  \u2022 \u001b[1mIntegrate Weights & Biases with cloud services and Docker images\u001b[0m : Users needed assistance with Weights & \n",
              "Biases authentication, particularly in cloud environments like AWS SageMaker, and with integrations involving \n",
              "custom Docker images. They sought guidance on secure key management, service accounts, and configuring SSO on the \n",
              "server-side W&B app, as well as examples for setting up W&B Launch with SageMaker and custom images \u001b[1m(\u001b[0mBYOI\u001b[1m)\u001b[0m. : \u001b[1;36m10\u001b[0m\n",
              "    + \u001b[1mConfigure and Integrate W&B with Cloud Services\u001b[0m : Users needed assistance with Weights & Biases \n",
              "authentication, particularly in cloud environments like AWS SageMaker, and with integrations involving custom \n",
              "Docker images. They sought guidance on secure key management, service accounts, and configuring SSO on the \n",
              "server-side W&B app, as well as examples for setting up W&B Launch with SageMaker and custom images \u001b[1m(\u001b[0mBYOI\u001b[1m)\u001b[0m. : \u001b[1;36m10\u001b[0m\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "====\n",
              "\n",
              "\n",
              "\u001b[1m(\u001b[0m\u001b[1m93cbee975f974856ba0e707921dcc2d0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m Manage and export data from Weights & Biases\u001b[0m : The clusters describe managing \n",
              "and exporting data, artifacts, access control, and permissions from the Weights & Biases platform, including team \n",
              "roles, collaboration features, and data formats, using the web panel or API, sometimes without an API key. It also \n",
              "includes understanding team roles, collaboration features, setting permissions, controlling access, and enabling \n",
              "collaboration with both W&B users and non-users, including handling sensitive data and resolving edit conflicts for\n",
              "multi-user access, admin roles, and SSO configurations : \u001b[1;36m53\u001b[0m\n",
              "\n",
              "  \u2022 \u001b[1mManage access control and permissions for Weights & Biases.\u001b[0m : The clusters describe managing and understanding \n",
              "weights and biases teams, projects, platform features, access control, permissions, and sharing capabilities within\n",
              "the Weights & Biases platform. It also includes understanding team roles, collaboration features, setting \n",
              "permissions, controlling access, and enabling collaboration with both W&B users and non-users, including handling \n",
              "sensitive data and resolving edit conflicts for multi-user access, admin roles, and SSO configurations.  : \u001b[1;36m40\u001b[0m\n",
              "    + \u001b[1mControl access and permissions in Weights & Biases\u001b[0m : The clusters describe managing and understanding weights\n",
              "and biases teams, projects, platform features, access control, permissions, and sharing capabilities within the \n",
              "Weights & Biases platform. It also includes understanding team roles, collaboration features, setting permissions, \n",
              "controlling access, and enabling collaboration with both W&B users and non-users, including handling sensitive data\n",
              "and resolving edit conflicts for multi-user access, admin roles, and SSO configurations.  : \u001b[1;36m40\u001b[0m\n",
              "\n",
              "\n",
              "  \u2022 \u001b[1mExport data and artifacts from Weights & Biases\u001b[0m : The clusters involve users seeking assistance with exporting \n",
              "various data formats and downloading artifacts from the Weights & Biases platform, often through the web panel or \n",
              "API, sometimes without an API key : \u001b[1;36m13\u001b[0m\n",
              "    + \u001b[1mExport data and artifacts from Weights & Biases\u001b[0m : The clusters involve users seeking assistance with \n",
              "exporting various data formats and downloading artifacts from the Weights & Biases platform, often through the web \n",
              "panel or API, sometimes without an API key : \u001b[1;36m13\u001b[0m\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "====\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get top-level clusters (those without parents)\n",
        "parent_clusters = [cluster for cluster in clusters if cluster.parent_id is None]\n",
        "\n",
        "# Format each cluster's info with name, description and number of chats\n",
        "formatted_clusters = []\n",
        "for parent in parent_clusters:\n",
        "    \n",
        "    # Add parent cluster info\n",
        "    cluster_info = (\n",
        "        f\"[bold]({parent.id}) {parent.name}[/bold] : {parent.description} : {len(parent.chat_ids)}\\n\"\n",
        "    )\n",
        "    \n",
        "    # Get and format child clusters\n",
        "    child_clusters = [c for c in clusters if c.parent_id == parent.id]\n",
        "    for child in child_clusters:\n",
        "        cluster_info += f\"\\n  \u2022 [bold]{child.name}[/bold] : {child.description} : {len(child.chat_ids)}\"\n",
        "        child_child_clusters = [c for c in clusters if c.parent_id == child.id]\n",
        "        for child_child in child_child_clusters:\n",
        "            if child_child.parent_id == child.id:\n",
        "                cluster_info += f\"\\n    + [bold]{child_child.name}[/bold] : {child_child.description} : {len(child_child.chat_ids)}\"\n",
        "        \n",
        "        cluster_info += \"\\n\\n\"\n",
        "    \n",
        "    formatted_clusters.append(cluster_info)\n",
        "    formatted_clusters.append(\"\\n====\\n\")\n",
        "\n",
        "# Join with newlines and print\n",
        "print(\"\\n\\n\".join(formatted_clusters))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\n### What You Learned\n\nIn this notebook, you learned how to create domain-specific summarization models that dramatically improve clustering quality. You discovered how to:\n\n- **Create custom summary models** using specialized prompts tailored to your domain\n- **Replace generic descriptions** with precise, feature-specific summaries\n- **Configure clustering parameters** to achieve optimal grouping results\n- **Compare clustering outcomes** between default and custom approaches\n\n### What We Accomplished\n\nWe built a custom `WnBSummaryModel` that addressed the key limitations from our initial clustering. By implementing domain-specific prompts that focus on W&B features and user intentions, we transformed our clustering results from generic topic groups into three actionable categories:\n\n1. **Access Controls** - Users asking about data handling and export in Weights and Biases\n2. **Deployment** - Users managing keys, service accounts, and integrating with Sagemaker and custom images  \n3. **Managing and Tracking Experiment Data** - Users working with Artifacts, generating visualizations, and integrating W&B with PyTorch and multi-GPU runs\n\nThis represents a significant upgrade from our previous clusters, providing much more specific and actionable information about user needs. The improved summaries eliminated the vagueness of descriptions like \"user seeks information about tracking\" and replaced them with precise insights about specific W&B workflows and pain points.\n\n### Next: Building Production Classifiers\n\nWhile our improved clustering gives us deep insights into historical query patterns, we need a way to act on these insights in real-time production environments. In the next notebook, \"Classifiers\", we'll bridge the gap between discovery and action by:\n\n- **Building production-ready classifiers** using the `instructor` library for real-time query categorization\n- **Creating automated labeling workflows** with weak supervision to scale annotation efforts\n- **Focusing on high-impact categories** like Artifacts, Integrations, and Visualizations that drive user satisfaction\n- **Iterating on classifier performance** using confusion matrices and systematic prompt engineering\n\nThis classifier will enable us to automatically categorize incoming queries, detect production drift when certain query types surge, and intelligently route questions to specialized retrieval systems\u2014transforming our clustering insights into a continuously improving RAG system."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}